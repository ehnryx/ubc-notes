\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx} %\usepackage[pdftex]{graphicx}
\usepackage{xcolor}
\usepackage{setspace}
%\usepackage{tikz}


\setlength\parindent{0pt}

\newtheorem{thm}{Theorem}[section]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{exe}[thm]{Exercise}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{pty}[thm]{Property}
\newtheorem{remark}[thm]{Remark}
\newtheorem{obs}[thm]{Observation}
\newcommand{\The}[2]{\begin{#1}#2\end{#1}}

\newcommand{\ord}[0]{\text{ord}}

% notes
\iftrue 
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\re}[1]{\frac{1}{#1}}
\newcommand{\half}[0]{\frac{1}{2}}
\newcommand{\ift}[0]{It follows that}
\newcommand{\cp}[1]{\overline{#1}}
\newcommand{\Note}[0]{\noindent\textbf{Note: }} 
\newcommand{\Claim}[0]{\noindent\textbf{Claim: }} 
\newcommand{\Lemma}[1]{\noindent\textbf{Lemma #1}: } %
\newcommand{\Ex}[0]{\noindent\textbf{Example: }} %
\newcommand{\Special}[0]{\noindent\textbf{Special case: }} %
\newcommand{\solution}[2]{\item[]\proof[Solution to #1] #2 \qedhere}
\newcommand{\legendre}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\dent}[0]{\hspace{0.5in}}
\fi

\newcommand{\sm}[0]{\setminus}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\nl}[0]{\vspace{12pt}}
\newcommand{\rng}[2]{#1,\dots,#2}
\newcommand{\srng}[3]{#1_#2,\dots,#1_#3}
\newcommand{\st}[0]{\text{ such that }}
\newcommand{\et}[0]{\text{ and }}
\newcommand{\then}[0]{\text{ then }}
\newcommand{\forsome}[0]{\text{ for some }}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\s}[0]{\sigma}
\newcommand{\e}[0]{\epsilon}

% misc
\newcommand{\abs}[1]{\left\lvert#1\right\rvert} %
% lcm ???
\DeclareMathOperator{\lcm}{lcm}
% blackboard bold
\newcommand{\RR}{\mathbb{R}}
\newcommand{\FF}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\CC}{\mathbb{C}}
% mathcal
\newcommand{\m}[1]{\mathcal{#1}}
% vectors
\newcommand{\vvec}[1]{\textbf{#1}} %
\newcommand{\ii}[0]{\vvec{i}} %
\newcommand{\jj}[0]{\vvec{j}} %
\newcommand{\kk}[0]{\vvec{k}} %
\newcommand{\hvec}[1]{\hat{\textbf{#1}}} %
\newcommand{\cvec}[3]{ %column vector
    \ensuremath{\left(\begin{array}{c}#1\\#2\\#3\end{array}\right)}}
\newcommand{\pfrac}[2]{\frac{\partial#1}{\partial#2}} %
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} %
% dotp roduct
\makeatletter
\newcommand*\dotp{\mathpalette\dotp@{.5}}
\newcommand*\dotp@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother
% divrg and curl
\newcommand{\divrg}[0]{\nabla\dotp} %
\newcommand{\curl}[0]{\nabla\times} %

\title{Math 539 Notes}
\author{Henry Xia}
%\date{15 September 2017}

\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 07

\section{Introduction}

Motivating questions (some statistics):
\begin{itemize}
\item the ``probability'' that a random number has some property
\item the ``distribution'' of some given multiplicative/additive function
\end{itemize}

Idea: we can answer the question for $\set{1,...,\floor{x}}$ for some parameter $x$. Then,
take the limit $x\to\infty$ for all natural numbers.

\subsection{Notation}

Let $g(x)\ge0$.

\begin{defn}
$O(g(x))$ means some unspecified function $u(x)$ such that $\abs{u(x)}\le cg(x)$ for some
constant $c>0$.
\end{defn}

\begin{example}
Show that $e^{2x}-1=2x+O(x^2)$ for $x=[-1,1]$.
\end{example}
\proof
Observe that $f(z)=e^{2z}-1-2z$ is analytic (and entire) and has a double zero at $z=0$
(one can check that $f(z)=f'(z)=0$. Hence, $g(z)=(e^{2z}-1-2z)/z^2$ has a removable
singularity at $z=0$, whence $g$ is analytic and entire. Let
$C=\max\set{\abs{g(z)}:\abs{z}\le1}$. Then 
\[
  \abs{g(z)}\le C \implies \abs{e^{2z}-1-2z} \le C\abs{z^2}
  \implies e^{2z}-1-2z = O(\abs{z}^2) .
\]
\qedhere

\begin{exe}
Show that $\sqrt{x+1}=\sqrt{x}+O(1/\sqrt{x})$ for $x\in[1,\infty)$.
\end{exe}

\begin{defn}
$f(x)\ll g(x)$ means $f(x)=O(g(x))$.
\end{defn}

\begin{exe}
Suppose that $f_1\ll g_1, f_2\ll g_2$, then $f_1+f_2\ll\max\set{g_1,g_2}$. \checkmark
\end{exe}

\begin{exe}
Let $f,g$ be continuous on $[0,\infty)$, and $f\ll g$ on $[123,\infty)$. Show that $f\ll g$
on $[0,\infty)$. \checkmark
\end{exe}

\begin{defn}
$f(x)\sim g(x)$ means $\lim\frac{f(x)}{g(x)}=1$.
\end{defn}

\begin{defn}
$f(x)=o(g(x))$ means $\lim\frac{f(x)}{g(x)}=0$.
\end{defn}

\begin{defn}
$f(x)=O_y(g(x))$ means $f,g$ depend on some parameter $y$, and the implicit constant
depends on $y$.
\end{defn}

\begin{exe}
For any $A,\epsilon>0$, show that $(\log x)^A \ll_{A,\epsilon} x^\epsilon$.
\end{exe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 09

\subsection{Riemann-Stieltjes Integral}

Appendix A in the book.

\begin{defn}
Some definitions for partitions
\begin{enumerate}
\item Let $\underline{x}=\set{x_0,...,x_N}$ be a partition of $[c,d]$ if
$c=x_0<\cdots<x_N=d$.
\item The mesh size $m(\underline{x})=\max_{1\le j\le N} x_j-x_{j-1}$.
\item Sample points $\xi_j\in[x_{j-1},x_j]$.
\end{enumerate}
\end{defn}

\begin{defn}[Riemann-Stieltjes Integral]
Given two functions $f(x)$ and $g(x)$, define the Riemann-Stieltjes integral as
\[
  \int_c^df(x)~dg(x) = \lim_{m(\underline{x})\to0} \sum_{j=1}^N
  f(\xi_j)(g(x_j)-g(x_{j-1})) .
\]
\end{defn}

\begin{remark}
Setting $g(x)=x$ gives the Riemann integral.
\end{remark}

\begin{thm}
Let $f(x)$ have bounded variation and let $g(x)$ be continuous on $[c,d]$, or vice versa.
Then $\int_c^df(x)~dg(x)$ exists.
\end{thm}

\begin{remark}
If a function is piecewise monotone, then it has bounded variation.
\end{remark}

\begin{example}
Given a sequence ${a_n}_{n\in\NN}$, define the summatory function $A(x)=\sum_{n\le x}a_n$.
Then, on any $[c,d]$, $A(x)$ is bounded, piecewise continuous and piecewise monotone.
Hence, the Riemann-Stieltjes integral exists when $g$ is continuous.
\end{example}

\begin{remark}
We present 3 facts that we will use.
\begin{enumerate}
\item If $A(x)$ is the summatory function as above, and $f(x)$ is continuous, then
\[
  \int_c^d f(x) ~dA(x) = \sum_{c<n\le d} a_nf(n) .
\]
\item (Integration by parts). If the integrals exist, then
\[
  \int_c^d f(x) ~dg(x) = \left.f(x)g(x)\right|_c^d - \int_c^d g(x) ~df(x) .
\]
\item If $f(x)$ is continuously differentiable, then
\[
  \int_c^d g(x) ~df(x) = \int_c^d g(x)f'(x) ~dx .
\]
\end{enumerate}
\end{remark}

\begin{example}[Summation by parts]
Consider $\sum_{n\le y}\frac{a_n}n$. Let $f(x)=1/x$, then we can write
\begin{multline*}
\sum_{n\le y} \frac{a_n}n
= \sum_{n\le y} a_n\cdot\frac1n = \int_0^y \frac1x ~dA(x)
= \left.\frac1xA(x)\right|_0^y - \int_0^y A(x) ~d\left(\frac1x\right)
= \frac{A(y)}y - \int_0^y A(x)\frac1{x^2} ~dx .
\end{multline*}
The final manipulation that we want to get is
\begin{equation}
\sum_{n\le y} a_nf(n) = A(y)f(y) - \int_0^y A(x)f'(x) ~dx .
\label{summation by parts}
\end{equation}
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 14

\section{Dirichlet Series}

A Dirichlet series is $\sum_{n=1}^\infty n^{-s}$.

Facts about Dirichlet series:
\begin{itemize}
\item converge in some right half-plane $\set{s\in\CC:\Re s>R}$ for some $R$ (possibly
$R=\pm\infty$).
\item Sometimes converge conditionally. Example: $\sum_{n=1}^\infty(-1)^n/n^{1/2}$.
\item $\left(\sum_{n=1}^\infty a_nn^{-s}\right)\left(\sum_{n=1}^\infty b_nn^{-s}\right)
= \sum_{n=1}^\infty c_nn^{-s}$ where $c=\sum_{de=n}a_db_e=\sum_{d\mid n}a_db_{e/d}$.
(multiplicative convolution)
\end{itemize}

Some notation: for $s\in\CC$, we write $s=\sigma+it$, that is $\sigma$ is the real part of
$s$, and $t$ is the imaginary part of $s$. Note that if $x>0$, then
$\abs{x^s}=\abs{x^\sigma}\abs{x^{it}}=\abs{x^\sigma}\abs{e^{it\log{x}}}=\abs{x^\sigma}$.

\begin{thm}
Let $\alpha(s)=\sum_{n=1}^\infty a_nn^{-s}$ be a Dirichlet series. Suppose that
$s_0\in\CC$ is such that $\alpha(s_0)$ converges. Then $\alpha(s)$ converges uniformly in
the sector $S=\set{s\in\CC:\sigma\ge\sigma_0,\abs{t-t_0}\le H\abs{\sigma-\sigma_0}}$ for
any $H>0$.
\end{thm}

\proof
WLOG, let $s_0=0$, otherwise we can do a change of variables.

Let $A(x)=\sum_{n\le x}a_n=\alpha(0)-R(x)$. Then, for $\sigma>0$,
\begin{align*}
\sum_{M<n\le N}a_n n^s &= \int_M^N x^{-s} ~dA(x) = \int_M^N x^{-s} ~d(\alpha(0)-R(x)) \\
&= \int_M^N x^{-s} ~d\alpha(0) - \int_M^N x^{-s} ~dR(x) = -\int_M^N x^{-s} ~dR(x) \\
&= \left.-x^{-s}R(x)\right|_M^N + \int_M^N R(x) ~d(x^{-s}) \\
&= R(M)M^{-s} - R(N)N^{-s} - s\int_M^N R(x)x^{-s-1} ~dx .
\end{align*}
Note that $R(N)N^{-s}\to0$ as $N\to\infty$, and that $R(x)x^{-s-1}\ll x^{-\sigma-1}$.
Hence, letting $N\to\infty$ gives
\[
  \sum_{M<n}a_nn^{-s} = R(M)M^{-s} - s\int_M^\infty R(x)x^{-s-1} ~dx
  \to 0 \text{ as } M \to \infty .
\]

Now, choose $M$ large such that $\abs{R(x)}<\epsilon$ for all $x\ge M$. Then,
\begin{align*}
  \abs{\sum_{n>M}a_nn^{-s}}
  &\le \epsilon M^{-\sigma} + \abs{s}\int_M^\infty \epsilon x^{-\sigma-1} ~dx \\
  &= \epsilon M^{-\s} + \left.\abs{s}\epsilon x^{-\s} \frac1{-\s} \right|_M^\infty \\
  &= \epsilon M^{-\s} + \abs{s} \epsilon \frac{M^{-\s}}{\s}
  = \frac{\epsilon}{M^\s} \left(1 + \frac{\abs{s}}{\s}\right) .
\end{align*}

Since $s\in S$, we have
\[
  \abs{s} = \sqrt{\s^2+t^2} \le \sqrt{\s^2 + \abs{H\s}^2} = \s\sqrt{1+H^2} ,
\]
so $\abs{\sum_{n>M}a_nn^{-s}} \le \epsilon(1+\sqrt{1+H^2})$ as $M\to\infty$. Observe that
the latter only depends on $H$, so the convergence is uniform.
\qedhere

\begin{cor}
If $\alpha(s_0)$ converges, then $\alpha(s)$ converges for all $s$ with $\sigma>\sigma_0$.
\end{cor}

\begin{cor}
If $\alpha(s_0)$ diverges, then $\alpha(s)$ diverges for all $s$ with $\sigma<\sigma_0$.
\end{cor}

\begin{remark}
The Dirichlet series $\alpha(s)$ has an abscissa of convergence $\sigma_c$ such that
$\alpha(s)$ converges if $\sigma>\sigma_c$, and diverges if $\sigma<\sigma_c$. It is
allowed to have $\sigma_c=\pm\infty$. Furthermore, $\alpha(s)$ converges locally uniformly
right of $\sigma_c$, whence $\alpha(s)$ is analytic.
\end{remark}

\begin{remark}
Observe that $\int_1^N x^{-s} ~dA(x) = \sum_{1<n\le N}a_nn^{-s} = \sum_{n=2}^Na_nn^{-s}$.
Sometimes we write $\int_{-1}^N$ to include the 1.
\end{remark}

\begin{thm}
Let $\alpha(s)=\sum_{n=1}^\infty a_nn^{-s}$ have an abscissa of convergence $\s_c\ge0$.
Then for $\s>\s_c$, we have $\alpha(s)=s\int_1^\infty A(x)x^{-s-1} ~dx$.
Moreover,
\[
  \limsup_{x\to\infty} \frac{\log\abs{A(x)}}{\log x} = \s_c .
\]
\end{thm}

\proof
Observe that
\begin{align*}
  \sum_{n=1}^N a_nn^{-s} &= \int_{1^-}^N x^{-s} ~dA(x)
  = \left.x^{-s}A(x)\right|_{1^-}^N - \int_{1^-}^N A(x) ~d(x^{-s}) \\
  &= A(N)N^{-s} - \int_{1^-}^N A(x)(-sx^{-s-1} ~dx)
  = A(N)N^{-s} + s\int_1^N A(x) x^{-s-1} ~dx .
\end{align*}
Observe that in the last line, we can replace $1^-$ with $1$ because the integrand is
bounded.

Define $\phi=\limsup_{x\to\infty}\frac{\log\abs{A(x)}}{\log x}$. We compare this to $\s_c$.

Let $\s=\phi+\epsilon$ for some $\e>0$. Then $\frac{\log\abs{A(x)}}{\log x}<\phi+\frac\e2$
for large $x$, so $A(x)\ll x^{\phi+\e/2}$. Then, $A(N)N^{-s}\ll N^{\phi+\e/2}N^{-(\phi+\e)}
= N^{-\e/2}$. Hence,
\[
  \int_N^\infty A(x)x^{-\s-1} ~dx \ll \int_N^\infty x^{-\phi+\e/2}x^{-(\phi+\e+1)} ~dx
  = \int_N^\infty x^{-1-\e/2} ~dx \ll N^{-\e/2} .
\]
It follows that
\[
  \sum_{n=1}^N a_nn^{-s}
  = O(N^{-\e/2}) + s\left(\int_1^\infty A(x)x{-s-1} ~dx + O(N^{-\e/2})\right) .
\]
Let $N\to\infty$ gives $s\int_1^\infty A(x)x^{-s-1} ~dx$ converges. Hence
$\sigma_c\le\phi$.

Conversely, let $\s_0=\s_c+\e$, and let
$R_0(x)=\sum_{n>x}a_nn^{-\s_0}=\alpha(\s_0)-\sum_{n\le x}a_nn^{-\s_0}$. Observe that
\[
  A(N) = -R_0(N)N^{\s_0} + \s_0\int_0^NR_0(x)x^{\s_0-1} ~dx .
\]
Since $\alpha(0)$ converges, $R_0(x)=o(1)$ so $R_0(x)\ll1$. Then
\[
  A(N) \ll 1 \cdot N^{\s_0} + \s_0\int_0^N 1\cdot x^{\s_0-1} ~dx
  = N^{\s_0}+N^{\s_c} \ll N^{\s_0} = N^{\s_c+\e} .
\]
Hence $\frac{\log\abs{A(x)}}{\log x} \ll \frac{(\s_c+\e)\log x}{\log x} = \s_c+\e$, so
$\phi\le\s_c+\e$. Take $\e\to0$, so $\phi\le\s_c$.
\qedhere


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 16




\end{document}
























