\documentclass[11pt]{article}
\usepackage{hyperref}
\usepackage[margin=1in]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{graphicx} %\usepackage[pdftex]{graphicx}
\usepackage{xcolor}
\usepackage{setspace}
%\usepackage{tikz}


\setlength\parindent{0pt}

\newtheorem{thm}{Theorem}[subsection]
\newtheorem{cor}[thm]{Corollary}
\newtheorem{lemma}[thm]{Lemma}
\theoremstyle{definition}
\newtheorem{defn}[thm]{Definition}
\newtheorem{example}[thm]{Example}
\newtheorem{exe}[thm]{Exercise}
\newtheorem{prop}[thm]{Proposition}
\newtheorem{pty}[thm]{Property}
\newtheorem{remark}[thm]{Remark}
\newtheorem{obs}[thm]{Observation}
\newcommand{\The}[2]{\begin{#1}#2\end{#1}}

\newcommand{\ord}[0]{\text{ord}}

% notes
\iftrue
\newcommand{\f}[2]{\frac{#1}{#2}}
\newcommand{\re}[1]{\frac{1}{#1}}
\newcommand{\half}[0]{\frac{1}{2}}
\newcommand{\ift}[0]{It follows that}
\newcommand{\cp}[1]{\overline{#1}}
\newcommand{\Note}[0]{\noindent\textbf{Note: }}
\newcommand{\Claim}[0]{\noindent\textbf{Claim: }}
\newcommand{\Lemma}[1]{\noindent\textbf{Lemma #1}: } %
\newcommand{\Ex}[0]{\noindent\textbf{Example: }} %
\newcommand{\Special}[0]{\noindent\textbf{Special case: }} %
\newcommand{\solution}[2]{\item[]\proof[Solution to #1] #2 \qedhere}
\newcommand{\legendre}[2]{\left(\frac{#1}{#2}\right)}
\newcommand{\dent}[0]{\hspace{0.5in}}
\fi

\newcommand{\sm}[0]{\setminus}
\newcommand{\set}[1]{\left\{ #1 \right\}}
\newcommand{\nl}[0]{\vspace{12pt}}
\newcommand{\rng}[2]{#1,\dots,#2}
\newcommand{\srng}[3]{#1_#2,\dots,#1_#3}
\newcommand{\st}[0]{\text{ such that }}
\newcommand{\et}[0]{\text{ and }}
\newcommand{\then}[0]{\text{ then }}
\newcommand{\forsome}[0]{\text{ for some }}
\newcommand{\floor}[1]{\lfloor #1 \rfloor}
\newcommand{\s}[0]{\sigma}
\newcommand{\e}[0]{\varepsilon}

% misc
\newcommand{\abs}[1]{\left\lvert#1\right\rvert} %
% lcm ???
\DeclareMathOperator{\lcm}{lcm}
% blackboard bold
\newcommand{\RR}{\mathbb{R}}
\newcommand{\FF}{\mathbb{R}}
\newcommand{\QQ}{\mathbb{Q}}
\newcommand{\ZZ}{\mathbb{Z}}
\newcommand{\NN}{\mathbb{N}}
\newcommand{\CC}{\mathbb{C}}
% mathcal
\newcommand{\m}[1]{\mathcal{#1}}
% vectors
\newcommand{\vvec}[1]{\textbf{#1}} %
\newcommand{\ii}[0]{\vvec{i}} %
\newcommand{\jj}[0]{\vvec{j}} %
\newcommand{\kk}[0]{\vvec{k}} %
\newcommand{\hvec}[1]{\hat{\textbf{#1}}} %
\newcommand{\cvec}[3]{ %column vector
    \ensuremath{\left(\begin{array}{c}#1\\#2\\#3\end{array}\right)}}
\newcommand{\pfrac}[2]{\frac{\partial#1}{\partial#2}} %
\newcommand{\norm}[1]{\left\lVert#1\right\rVert} %
% dotp roduct
\makeatletter
\newcommand*\dotp{\mathpalette\dotp@{.5}}
\newcommand*\dotp@[2]{\mathbin{\vcenter{\hbox{\scalebox{#2}{$\m@th#1\bullet$}}}}}
\makeatother
% divrg and curl
\newcommand{\divrg}[0]{\nabla\dotp} %
\newcommand{\curl}[0]{\nabla\times} %

\title{Math 539 Notes}
\author{Henry Xia}
%\date{15 September 2017}

\begin{document}

\maketitle

\tableofcontents

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 07

\section{Introduction}

Motivating questions (some statistics):
\begin{itemize}
\item the ``probability'' that a random number has some property
\item the ``distribution'' of some given multiplicative/additive function
\end{itemize}

Idea: we can answer the question for $\set{1,...,\floor{x}}$ for some parameter $x$. Then,
take the limit $x\to\infty$ for all natural numbers.

\subsection{Notation}

Let $g(x)\ge0$.

\begin{defn}
$O(g(x))$ means some unspecified function $u(x)$ such that $\abs{u(x)}\le cg(x)$ for some
constant $c>0$.
\end{defn}

\begin{example}
Show that $e^{2x}-1=2x+O(x^2)$ for $x=[-1,1]$.
\end{example}
\proof
Observe that $f(z)=e^{2z}-1-2z$ is analytic (and entire) and has a double zero at $z=0$
(one can check that $f(z)=f'(z)=0$. Hence, $g(z)=(e^{2z}-1-2z)/z^2$ has a removable
singularity at $z=0$, whence $g$ is analytic and entire. Let
$C=\max\set{\abs{g(z)}:\abs{z}\le1}$. Then
\[
  \abs{g(z)}\le C \implies \abs{e^{2z}-1-2z} \le C\abs{z^2}
  \implies e^{2z}-1-2z = O(\abs{z}^2) .
\]
\qedhere

\begin{exe}
Show that $\sqrt{x+1}=\sqrt{x}+O(1/\sqrt{x})$ for $x\in[1,\infty)$.
\end{exe}

\begin{defn}
$f(x)\ll g(x)$ means $f(x)=O(g(x))$.
\end{defn}

\begin{exe}
Suppose that $f_1\ll g_1, f_2\ll g_2$, then $f_1+f_2\ll\max\set{g_1,g_2}$. \checkmark
\end{exe}

\begin{exe}
Let $f,g$ be continuous on $[0,\infty)$, and $f\ll g$ on $[123,\infty)$. Show that $f\ll g$
on $[0,\infty)$. \checkmark
\end{exe}

\begin{defn}
$f(x)\sim g(x)$ means $\lim\frac{f(x)}{g(x)}=1$.
\end{defn}

\begin{defn}
$f(x)=o(g(x))$ means $\lim\frac{f(x)}{g(x)}=0$.
\end{defn}

\begin{defn}
$f(x)=O_y(g(x))$ means $f,g$ depend on some parameter $y$, and the implicit constant
depends on $y$.
\end{defn}

\begin{exe}
For any $A,\epsilon>0$, show that $(\log x)^A \ll_{A,\epsilon} x^\epsilon$.
\end{exe}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 09

\subsection{Riemann-Stieltjes Integral}

Appendix A in the book.

\begin{defn}
Some definitions for partitions
\begin{enumerate}
\item Let $\underline{x}=\set{x_0,...,x_N}$ be a partition of $[c,d]$ if
$c=x_0<\cdots<x_N=d$.
\item The mesh size $m(\underline{x})=\max_{1\le j\le N} x_j-x_{j-1}$.
\item Sample points $\xi_j\in[x_{j-1},x_j]$.
\end{enumerate}
\end{defn}

\begin{defn}[Riemann-Stieltjes Integral]
Given two functions $f(x)$ and $g(x)$, define the Riemann-Stieltjes integral as
\[
  \int_c^df(x)~dg(x) = \lim_{m(\underline{x})\to0} \sum_{j=1}^N
  f(\xi_j)(g(x_j)-g(x_{j-1})) .
\]
\end{defn}

\begin{remark}
Setting $g(x)=x$ gives the Riemann integral.
\end{remark}

\begin{thm}
Let $f(x)$ have bounded variation and let $g(x)$ be continuous on $[c,d]$, or vice versa.
Then $\int_c^df(x)~dg(x)$ exists.
\end{thm}

\begin{remark}
If a function is piecewise monotone, then it has bounded variation.
\end{remark}

\begin{example}
Given a sequence ${a_n}_{n\in\NN}$, define the summatory function $A(x)=\sum_{n\le x}a_n$.
Then, on any $[c,d]$, $A(x)$ is bounded, piecewise continuous and piecewise monotone.
Hence, the Riemann-Stieltjes integral exists when $g$ is continuous.
\end{example}

\begin{remark}
We present 3 facts that we will use.
\begin{enumerate}
\item If $A(x)$ is the summatory function as above, and $f(x)$ is continuous, then
\[
  \int_c^d f(x) ~dA(x) = \sum_{c<n\le d} a_nf(n) .
\]
\item (Integration by parts). If the integrals exist, then
\[
  \int_c^d f(x) ~dg(x) = \left.f(x)g(x)\right|_c^d - \int_c^d g(x) ~df(x) .
\]
\item If $f(x)$ is continuously differentiable, then
\[
  \int_c^d g(x) ~df(x) = \int_c^d g(x)f'(x) ~dx .
\]
\end{enumerate}
\end{remark}

\begin{example}[Summation by parts]
Consider $\sum_{n\le y}\frac{a_n}n$. Let $f(x)=1/x$, then we can write
\begin{multline*}
\sum_{n\le y} \frac{a_n}n
= \sum_{n\le y} a_n\cdot\frac1n = \int_0^y \frac1x ~dA(x)
= \left.\frac1xA(x)\right|_0^y - \int_0^y A(x) ~d\left(\frac1x\right)
= \frac{A(y)}y - \int_0^y A(x)\frac1{x^2} ~dx .
\end{multline*}
The final manipulation that we want to get is
\begin{equation}
\sum_{n\le y} a_nf(n) = A(y)f(y) - \int_0^y A(x)f'(x) ~dx .
\label{summation by parts}
\end{equation}
\end{example}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 14

\section{Dirichlet Series}

A Dirichlet series is $\sum_{n=1}^\infty n^{-s}$.

Facts about Dirichlet series:
\begin{itemize}
\item converge in some right half-plane $\set{s\in\CC:\Re s>R}$ for some $R$ (possibly
$R=\pm\infty$).
\item Sometimes converge conditionally. Example: $\sum_{n=1}^\infty(-1)^n/n^{1/2}$.
\item $\left(\sum_{n=1}^\infty a_nn^{-s}\right)\left(\sum_{n=1}^\infty b_nn^{-s}\right)
= \sum_{n=1}^\infty c_nn^{-s}$ where $c=\sum_{de=n}a_db_e=\sum_{d|n}a_db_{e/d}$.
(multiplicative convolution)
\end{itemize}

Some notation: for $s\in\CC$, we write $s=\sigma+it$, that is $\sigma$ is the real part of
$s$, and $t$ is the imaginary part of $s$. Note that if $x>0$, then
$\abs{x^s}=\abs{x^\sigma}\abs{x^{it}}=\abs{x^\sigma}\abs{e^{it\log{x}}}=\abs{x^\sigma}$.

\begin{thm}
Let $\alpha(s)=\sum_{n=1}^\infty a_nn^{-s}$ be a Dirichlet series. Suppose that
$s_0\in\CC$ is such that $\alpha(s_0)$ converges. Then $\alpha(s)$ converges uniformly in
the sector $S=\set{s\in\CC:\sigma\ge\sigma_0,\abs{t-t_0}\le H\abs{\sigma-\sigma_0}}$ for
any $H>0$.
\end{thm}

\proof
WLOG, let $s_0=0$, otherwise we can do a change of variables.

Let $A(x)=\sum_{n\le x}a_n=\alpha(0)-R(x)$. Then, for $\sigma>0$,
\begin{align*}
\sum_{M<n\le N}a_n n^s &= \int_M^N x^{-s} ~dA(x) = \int_M^N x^{-s} ~d(\alpha(0)-R(x)) \\
&= \int_M^N x^{-s} ~d\alpha(0) - \int_M^N x^{-s} ~dR(x) = -\int_M^N x^{-s} ~dR(x) \\
&= \left.-x^{-s}R(x)\right|_M^N + \int_M^N R(x) ~d(x^{-s}) \\
&= R(M)M^{-s} - R(N)N^{-s} - s\int_M^N R(x)x^{-s-1} ~dx .
\end{align*}
Note that $R(N)N^{-s}\to0$ as $N\to\infty$, and that $R(x)x^{-s-1}\ll x^{-\sigma-1}$.
Hence, letting $N\to\infty$ gives
\[
  \sum_{M<n}a_nn^{-s} = R(M)M^{-s} - s\int_M^\infty R(x)x^{-s-1} ~dx
  \to 0 \text{ as } M \to \infty .
\]

Now, choose $M$ large such that $\abs{R(x)}<\epsilon$ for all $x\ge M$. Then,
\begin{align*}
  \abs{\sum_{n>M}a_nn^{-s}}
  &\le \epsilon M^{-\sigma} + \abs{s}\int_M^\infty \epsilon x^{-\sigma-1} ~dx \\
  &= \epsilon M^{-\s} + \left.\abs{s}\epsilon x^{-\s} \frac1{-\s} \right|_M^\infty \\
  &= \epsilon M^{-\s} + \abs{s} \epsilon \frac{M^{-\s}}{\s}
  = \frac{\epsilon}{M^\s} \left(1 + \frac{\abs{s}}{\s}\right) .
\end{align*}

Since $s\in S$, we have
\[
  \abs{s} = \sqrt{\s^2+t^2} \le \sqrt{\s^2 + \abs{H\s}^2} = \s\sqrt{1+H^2} ,
\]
so $\abs{\sum_{n>M}a_nn^{-s}} \le \epsilon(1+\sqrt{1+H^2})$ as $M\to\infty$. Observe that
the latter only depends on $H$, so the convergence is uniform.
\qedhere

\begin{cor}
If $\alpha(s_0)$ converges, then $\alpha(s)$ converges for all $s$ with $\sigma>\sigma_0$.
\end{cor}

\begin{cor}
If $\alpha(s_0)$ diverges, then $\alpha(s)$ diverges for all $s$ with $\sigma<\sigma_0$.
\end{cor}

\begin{remark}
The Dirichlet series $\alpha(s)$ has an abscissa of convergence $\sigma_c$ such that
$\alpha(s)$ converges if $\sigma>\sigma_c$, and diverges if $\sigma<\sigma_c$. It is
allowed to have $\sigma_c=\pm\infty$. Furthermore, $\alpha(s)$ converges locally uniformly
right of $\sigma_c$, whence $\alpha(s)$ is analytic.
\end{remark}

\begin{remark}
Observe that $\int_1^N x^{-s} ~dA(x) = \sum_{1<n\le N}a_nn^{-s} = \sum_{n=2}^Na_nn^{-s}$.
Sometimes we write $\int_{-1}^N$ to include the 1.
\end{remark}

\begin{thm}
Let $\alpha(s)=\sum_{n=1}^\infty a_nn^{-s}$ have an abscissa of convergence $\s_c\ge0$.
Then for $\s>\s_c$, we have $\alpha(s)=s\int_1^\infty A(x)x^{-s-1} ~dx$.
Moreover,
\[
  \limsup_{x\to\infty} \frac{\log\abs{A(x)}}{\log x} = \s_c .
\]
\end{thm}

\proof
Observe that
\begin{align*}
  \sum_{n=1}^N a_nn^{-s} &= \int_{1^-}^N x^{-s} ~dA(x)
  = \left.x^{-s}A(x)\right|_{1^-}^N - \int_{1^-}^N A(x) ~d(x^{-s}) \\
  &= A(N)N^{-s} - \int_{1^-}^N A(x)(-sx^{-s-1} ~dx)
  = A(N)N^{-s} + s\int_1^N A(x) x^{-s-1} ~dx .
\end{align*}
Observe that in the last line, we can replace $1^-$ with $1$ because the integrand is
bounded.

Define $\phi=\limsup_{x\to\infty}\frac{\log\abs{A(x)}}{\log x}$. We compare this to $\s_c$.

Let $\s=\phi+\epsilon$ for some $\e>0$. Then $\frac{\log\abs{A(x)}}{\log x}<\phi+\frac\e2$
for large $x$, so $A(x)\ll x^{\phi+\e/2}$. Then, $A(N)N^{-s}\ll N^{\phi+\e/2}N^{-(\phi+\e)}
= N^{-\e/2}$. Hence,
\[
  \int_N^\infty A(x)x^{-\s-1} ~dx \ll \int_N^\infty x^{-\phi+\e/2}x^{-(\phi+\e+1)} ~dx
  = \int_N^\infty x^{-1-\e/2} ~dx \ll N^{-\e/2} .
\]
It follows that
\[
  \sum_{n=1}^N a_nn^{-s}
  = O(N^{-\e/2}) + s\left(\int_1^\infty A(x)x{-s-1} ~dx + O(N^{-\e/2})\right) .
\]
Let $N\to\infty$ gives $s\int_1^\infty A(x)x^{-s-1} ~dx$ converges. Hence
$\sigma_c\le\phi$.

Conversely, let $\s_0=\s_c+\e$, and let
$R_0(x)=\sum_{n>x}a_nn^{-\s_0}=\alpha(\s_0)-\sum_{n\le x}a_nn^{-\s_0}$. Observe that
\[
  A(N) = -R_0(N)N^{\s_0} + \s_0\int_0^NR_0(x)x^{\s_0-1} ~dx .
\]
Since $\alpha(0)$ converges, $R_0(x)=o(1)$ so $R_0(x)\ll1$. Then
\[
  A(N) \ll 1 \cdot N^{\s_0} + \s_0\int_0^N 1\cdot x^{\s_0-1} ~dx
  = N^{\s_0}+N^{\s_c} \ll N^{\s_0} = N^{\s_c+\e} .
\]
Hence $\frac{\log\abs{A(x)}}{\log x} \ll \frac{(\s_c+\e)\log x}{\log x} = \s_c+\e$, so
$\phi\le\s_c+\e$. Take $\e\to0$, so $\phi\le\s_c$.
\qedhere


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 16

\begin{defn}
The abscissa of absolute convergence is
$\s_a=\inf\set{\s\in\RR:\sum_{n=1}^\infty\abs{a_n}n^{-\s}\text{ converges}}$.
\end{defn}

\begin{example}
Let $\eta(s)=\sum_{n=1}^\infty(-1)^{n-1}n^{-s}$. Observe that $\s_c=0$ by the alternating
series test. However, we only have absolute convergence when $\s>1$, so the abscissa of
absolute convergence is $\s_a=1$.
\end{example}

\begin{remark}
When $a_n\ge0$ for all $n\in\NN$, we have $\s_c=\s_a$.
\end{remark}

\begin{thm}
For any Dirichlet series $\alpha(s)$, we have $\s_c\le\s_a\le\s_c+1$.
\end{thm}
\proof
The first inequality is trivial.

Let $\s=\s_c+1+\e$ where $\e>0$. We show that $\alpha(\s)$ converges absolutely. Note that
$\alpha(s)$ converges at $s=\s_c+\e/2$, that is
\[
  a_nn^{-(\s_c+\e/2)} = o(1) \implies a_nn^{-(\s_c+\e/2)} \ll 1 .
\]
Then,
\[
  \sum_{n=1}^\infty\abs{a_n}n^{-(\s_c+1+\e)}
  = \sum_{n=1}^\infty\abs{a_nn^{-(\s_c+\e/2)}}n^{-(1+\e/2)}
  \ll \sum_{n=1}^\infty n^{-(1+\e/2)} \ll 1 .
\]
It follows that $\alpha(\s)$ converges absolutely for all $\e>0$.
\qedhere

\begin{thm}[Landau's Theorem]
Let $\alpha(s)=\sum_{n=1}^\infty a_nn^{-s}$ with $\s_c<\infty$. If $a_n\ge0$ for each
$n\in\NN$. Then, $\alpha(s)$ has a singularity at $s=\s_c$.
\end{thm}
\proof
Suppose that there does not exist a singularity at $s=\s_c$. Then, there exists an analytic
continuation of $\alpha$ to $C=\set{s\in\CC:\abs{s-\s_c}<\delta}$.

Let $z=\s_c-\frac14\delta$ and let $w=\s_c+\frac34\delta$. Let
$D=\set{s\in\CC:\abs{s-w}<\frac54\delta}$. Observe that $D\subset C\cup\set{s\in\CC:\s>0}$,
so $\alpha$ has an analytic continuation to $D$. Let $P(s)$ be the power series of $\alpha$
centered at $w$. Observe that $z\in D$, so it suffices to show that $P(z)=\alpha(z)$,
whence we contradict the assumption that the abscissa of convergence is $\s_c$. Note that
\begin{align*}
P(z) &= \sum_{k=0}^\infty \frac{\alpha^{(k)}(w)}{k!} (z-w)^k \\
&= \sum_{k=0}^\infty \frac{1}{k!}(z-w)^k \sum_{n=1}^\infty a_n(-\log n)^k n^{-w}
~~~~~~~\text{we can differentiate termwise for }\alpha^{(k)}(w) \\
&= \sum_{k=1}^\infty \frac{1}{k!}(w-z)^k \sum_{n=1}^\infty a_n(\log n)^k n^{-w}
~~~~~~~\text{where the terms are all nonnegative} \\
&= \sum_{n=1}^\infty a_nn^{-w} \sum_{k=1}^\infty \frac{1}{k!}(w-z)^k(\log n)^k \\
&= \sum_{n=1}^\infty a_nn^{-w} e^{(w-z)\log n} = \sum_{n=1}^\infty a_nn^{-z} .
\end{align*}
It follows that $\alpha(z)$ converges left of $\s_c$, which is a contradiction.
\qedhere


% TODO @ sec 1.3 of text on multiplicative stuff
\subsection{Dirichlet convolutions}

Motivating question: are these calculations legitimate?
\begin{itemize}
\item $\zeta(s)^2 = \sum_{l,m=1}^\infty (lm)^{-s} = \sum_{n=1}^\infty d(n)n^{-s}$.
\item $\zeta(s) = \prod_{p\text{ prime}}(1+p^{-s}+p^{-2s}+\cdots) = \prod_{p\text{ prime}}
(1-p^{-s})^{-1}$ .
\end{itemize}

\begin{defn}
  Let $a=\set{a_n}$, $b=\set{b_n}$ be sequences. The Dirichlet/multiplicative convolution
  $a*b$ by $c=\set{c_n}$ where $c_n = \sum_{de=n}a_db_e = \sum_{d|n}a_db_{n/d}$.
\end{defn}

\begin{thm}
Let $\alpha(s)=\sum_{n=1}^\infty a_nn^{-s}$, let $\beta(s)=\sum_{n=1}^\infty b_nn^{-s}$,
and let $\gamma(s)=\sum_{n=1}^\infty c_nn^{-s}$. If $s\in\CC$ is such that $\alpha(s)$ and
$\beta(s)$ converge absolutely, and if $c=a*b$, then $\gamma(s)$ converges absolutely and
$\gamma(s)=\alpha(s)\beta(s)$.
\end{thm}

\begin{example}
Observe that $d(n)=(1*1)(n)$.
\end{example}

\begin{example}
Let $M(s)=\sum_{n=1}^\infty\mu(n)n^{-s}$, where $\mu$ is the M\"obius function which
defined as
\[
  \mu(n) = \begin{cases}
  0 &\text{if } n \text{ is not square free} \\
  1 &\text{if } n \text{ has an even number of prime divisors} \\
  -1 &\text{if } n \text{ has an odd number of prime divisors}
  \end{cases} .
\]
Equivalently, we can define $\mu$ as the function that satisfies
\[
  \sum_{d|n}\mu(d) = \begin{cases}
  1 &\text{if } n = 1 \\
  0 &\text{if } n > 1
  \end{cases} .
\]
Observe that $M(s)\zeta(s) = \sum_{n=1}^\infty(\mu*1)(n)n^{-s} = 1$ for $\s>1$, since
$(\mu*1)(n)=\sum_{d|n}\mu(d)$. It follows that $M(s)=1/\zeta(s)$.

Since the abscissa of convergence of $M$ is $\s_c=1$, we get that $\zeta(s)$ has no zeroes
when $\s>1$.
\end{example}

\begin{example}[M\"obius Inversion Formula]
Write $F(s)=\sum_{n=1}^\infty f(n)n^{-s}$ and $G(s)=\sum_{n=1}^\infty g(n)n^{-s}$. Then
\begin{align*}
F(s)\zeta(s) = G(s) &\iff F(s) = \frac{G(s)}{\zeta(s)} = G(s)M(s) \\
(f*1)(n) = g(n) &\iff f(n) = (g*\mu)(n) \\
\sum_{d|n} f(n) = g(n) &\iff f(n) = \sum_{d|n} g(d)\mu\left(\frac{n}{d}\right) .
\end{align*}
\end{example}

\begin{example}
It is known that $\sum_{d|n}\phi(d)=n$. This gives $(\phi*1)(n)=\sum_{d|n}\phi(d)=n$. Then,
for $\s>2$, we have
\[
\left(\sum_{n=1}^\infty \phi(n)n^{-s}\right) \left(\sum_{n=1}^\infty n^{-s}\right)
= \sum_{n=1}^\infty n\cdot n^{-s} = \zeta(s-1) .
\]
This gives $\sum_{n=1}^\infty\phi(n)n^{-s}=\zeta(s-1)/\zeta(s)$.
\end{example}

\begin{exe}
Let $\sigma_1(n)=\sum_{d|n}d$. Show that
$\sum_{n=1}^\infty\sigma_1(n)n^{-s}=\zeta(s-1)\zeta(s)$.
\end{exe}

\begin{defn}
A function $f$ is multiplicative if $f(m)f(n)=f(mn)$ if $\gcd(m,n)=1$.
\end{defn}

\begin{defn}
A number $n$ is $y$-friable if $p\mid n \implies p\le y$.
\end{defn}

\begin{thm}
Let $f$ be a multiplicative function, and let $F(s)=\sum_{n=1}^\infty f(n)n^{-s}$. If
$\sum_{n=1}^\infty\abs{f(n)}n^{-\s}$ converges, we have the Euler product
\[
  F(s) = \prod_{p\text{ prime}} (1 + f(p)p^{-s} + f(p^2)p^{-2s} + \cdots) .
\]
\end{thm}
\proof
Let $\s>\s_a$. Then, for all $p$, we have
\[
\abs{1 + f(p)p^{-s} + f(p^2)p^{-2s} + \cdots}
\le 1 + \abs{f(p)}p^{-s} + \abs{f(p)}p^{-2s} + \cdots
\le \sum_{n=1}^\infty \abs{f(n)}n^{-s} .
\]
Since the above converges, we can rearrange the finite product
\[
\prod_{p\le y} (1 + f(p)p^{-s} + f(p^2)p^{-2s} + \cdots)
= \sum_{n~y\text{-friable}} f(n)n^{-s} .
\]
Now, we can compute
\begin{align*}
\abs{F(s) - \prod_{p\le y} (1 + f(p)p^{-s} + f(p^2)p^{-2s} + \cdots)}
&= \abs{F(s) - \sum_{n~y\text{-friable}} f(n)n^{-s}} \\
&= \abs{\sum_{n~\text{not}~y\text{-friable}} f(n)n^{-s}} \\
&\le \sum_{n>y} \abs{f(n)}n^{-s} = o(1) .
\end{align*}
The tail goes to 0, so the theorem is proved.
\qedhere

\begin{remark}
Almost the same proof shows that the Euler product converges absolutely. In particular, it
is nonzero (unless an individual factor is zero). Note that the convergence of a product is
defined as the convergence of the sum of logs.
\end{remark}

\begin{example}
Note that $\mu$ is multiplicative, so $M(s)=\prod_{p\text{ prime}}(1-p^{-s})$.
\end{example}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 21





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 23





%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% 2020 01 28







\end{document}
























